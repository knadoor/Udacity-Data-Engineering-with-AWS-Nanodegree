# 2 - Data warehouse with AWS

## Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data 
resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I've been tasked with building an ETL pipeline that extracts their data from S3, stages them in AWS Redshift, and transforms data 
into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to. The dimensional 
tables is required to follow Star Schema.

## Project Description
In this project, I'll be applying what I've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. 
To complete the project, I will be loading data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from 
these staging tables.

## Project Datasets
There are 3 files that are stored on an S3 bucket that I'll be working with:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`
* Meta information for AWS: `s3://udacity-dend/log_json_path.json`, this is used to correctly load `s3://udacity-dend/log_data`

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.<br>
`song_data/A/B/C/TRABCEI128F424C983.json`<br>
`song_data/A/A/B/TRAABJL12903CDCF1A.json`<br>
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like:<br>
`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

### Log Dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset I worked with were partitioned by year and month. For example, here are file paths to two files in this dataset:<br>
`log_data/2018/11/2018-11-12-events.json`<br>
`log_data/2018/11/2018-11-13-events.json`<br>
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like:
![image](https://user-images.githubusercontent.com/8476589/211443662-82950ca5-4835-4e1b-bd5c-2744d56e7b83.png)

### Log JSON Meta information for AWS
This is the contents of the `log_json_path.json` file:<br>
![image](https://user-images.githubusercontent.com/8476589/211443876-7542443d-2889-4938-aa00-447e56f54f90.png)

## Database Schemas for Song play analysis
A star schema was developed for performing optimized queries on song play analysis. It includes the following tables:

### Staging tables
* staging_events and staging_songs

### Fact table
* <b>songplays - </b>records in event data associated with song plays i.e. records with page `NextSong`

### Dimension tables
* <b>users - </b>users in the app
* <b>songs - </b>songs in music database
* <b>artists - </b>artists in music database
* <b>time - </b>timestamps of records in *songplays* broken down into specific units

### Entity Relationship Diagram
<img width="877" alt="image" src="https://user-images.githubusercontent.com/8476589/211444952-b43397c8-5ee2-4f24-bd1e-adfa6d7ca74e.png">
<img width="474" alt="image" src="https://user-images.githubusercontent.com/8476589/211445017-ce7fad40-fdca-4543-a227-f1e64178df76.png">

## Project files
* `create_redshift_cluster.py` This file connects to AWS Services via the boto3 Python library to create a Redshift cluster
* `create_tables.py` This file creates the staging and dimensional tables in Redshift, it drops any prior created tables
* `etl.py` This file extracts data from S3 and puts it into the staging tables in Redshift, before loading it into the dimensional tables in Redshift
* `sql_queries.py` This file contains the SQL queries that makes the ETL in `etl.py` possible
* `dwh.cfg` This is a configuration file that contains information on your generated AWS `Access Key` and `Secret` as well as Redshift cluster details such as the `host` (the endpoint), `dbname`, `port` (defaut is 5439 for Redshift), `user` etc. This file also contains the `ARN`, `region` (i.e., us-west-2) and `S3` details

## Instructions on running the project
1 - Open AWS IAM (Identity Access Management) and create a new IAM user. In the AWS access type section make sure you choose both `Programmatic access` and `AWS Management Console access`<br>
2 - Give the user the following permissions from the `Attach existing policies directly` list: `AdministratorAccess`, `AmazonRedshiftFullAccess`, `AmazonRedshiftReadOnlyAccess`, `AmazonS3ReadOnlyAccess`, `AmazonRedshiftQueryEditor`, `AmazonRedshiftQueryEditorV2FullAccess`<br>
3 - Create a IAM role granting access to relevant AWS S3 and Redshift permissions
4 - Take note of the access key and secret and edit the `dwh.cfg` accordingly to include the required info<br>
5 - Run the `create_redshift_cluster.py` script to create the Redshift cluster<br>
6 - Run the `create_tables.py` script to create the staging and dimensional tables in the cluster<br>
7 - Run the `etl.py` script to perform the ETL from S3 to the staging tables, and then from staging to dimensional analytics tables<br>
8 - Open the `Redshift Query Editor` for the cluster to check the table data<br>
<br>
For reference, the `staging_events` table has `8056` records, and the `staging_songs` table has `14896` records.

## Screenshots of the code runs
Running the `create_redshift_cluster.py` script<br>
![image](https://user-images.githubusercontent.com/8476589/211447733-c334bb9a-aa61-45cf-bed4-8f1ff757fbc2.png)<br>

Running `create_tables.py` script<br>
![image](https://user-images.githubusercontent.com/8476589/211447436-f9605b22-28a7-4849-ae8d-bb8dc1a68301.png)<br>

Running the `etl.py` script<br>
![image](https://user-images.githubusercontent.com/8476589/211447615-371a66c6-225b-4cb4-8f13-98505c6df9f2.png)<br>

## References
The following pages have been helpful in the development of this project. I've included these here for future reference.<br>
* [AWS EXTRACT function](https://docs.aws.amazon.com/redshift/latest/dg/r_EXTRACT_function.html)<br>
* [AWS DATE_PART function](https://docs.aws.amazon.com/redshift/latest/dg/r_DATE_PART_function.html)<br>
* [AWS Date parts for date or timestamp functions](https://docs.aws.amazon.com/redshift/latest/dg/r_Dateparts_for_datetime_functions.html)<br>
* [Stackoverflow - How to convert epoch to datetime redshift?](https://stackoverflow.com/questions/39815425/how-to-convert-epoch-to-datetime-redshift)<br>
* [Stackoverflow - Storing sex (gender) in database](https://stackoverflow.com/questions/4175878/storing-sex-gender-in-database)<br>
* [Stackoverflow - Star-schema naming conventions](https://stackoverflow.com/questions/1716220/star-schema-naming-conventions)<br>
* [Stackoverflow - What datatype to use when storing latitude and longitude data in SQL databases?](https://stackoverflow.com/questions/1196415/what-datatype-to-use-when-storing-latitude-and-longitude-data-in-sql-databases)<br>
* [Stackoverflow page for resolving the error ImportError: No module named 'psycopg2._psycopg'](https://stackoverflow.com/questions/36103034/importerror-no-module-named-psycopg2-psycopg)




